#!/bin/bash

# You may want to run it like this: time ./Bad-HOMEPAGE-Finder.sh > ~/Bad-HOMEPAGEs-on-`date +%m-%d-%y`.txt

# Then you can, in another terminal, watch the output:  tail -f ~/Bad-HOMEPAGEs-on-`date +%m-%d-%y`.txt

# https://github.com/write2david/Portage-HOMEPAGE-Checker/

# Current version of this script is available here:
# https://github.com/write2david/Portage-HOMEPAGE-Checker/raw/master/Find-Portage-HOMEPAGE-Problems.sh 

echo
echo 
echo "Welcome to the Portage Tree Homepage Checker"
echo 
echo "   The current version is available at:"
echo "   https://github.com/write2david/Portage-HOMEPAGE-Checker"
echo
echo "   This script depends on the 'sys-process/time' package."
echo
echo
echo "We will test all HOMEPAGE variables in your Portage tree."
echo "     (So, make sure you have recently run 'emerge --sync')"
echo "     We will find HOMEPAGES (and sometimes SRC_URI's) that don't load."
echo
echo




# STEP 1

# Find all ebuilds and grep their HOMEPAGE line
# Remove "HOMEPAGE="
# Remove all double-quotes
# Sometimes more than one URL is listed on the same HOMEPAGE line, so convert spaces to new lines
# Sometimes people add stuff to HOMEPAGE besides the URL, like comments -- so get rid of all entries that don't include "http://"]


echo
echo
echo "1) Now building a list of all HOMEPAGE website addresses in the Portage tree."
echo "     (this will take a little while)..."
echo


# Let's pull all the HOMEPAGE lines out of all the ebuilds in the Portage tree

# Homepages may be commented out, so in the following line, we grep for HOMEPAGE only if it is at the beginning of the line (which means there is no # before it).  See http://bugs.gentoo.org/show_bug.cgi?id=366957

# We're also measuring the time it takes and saving that time to a file 
/usr/bin/time -f %E -o /tmp/PTHC-Find-Homepages.txt find /usr/portage/ -name '*.ebuild' -type f -exec grep '^HOMEPAGE' '{}' \; | sed 's/HOMEPAGE=//' | sed 's/"//g' | sed 's/\ /\n/g' | grep http:// > /tmp/PortageHomepages-Unsorted.txt

# Now we will alphabetize them and remove duplicates 
sort /tmp/PortageHomepages-Unsorted.txt | uniq > /tmp/PortageHomepages.txt


echo "     Number of unique HOMEPAGE URL's in Portage tree: `wc -l /tmp/PortageHomepages.txt | awk '{ print $1}'`"
echo
# The next line uses a sed command to remove the decimal point in the seconds (no need to show partial seconds)
# And so it searches for the string "period with any two characters after the period"  -- the single character wildcar
#   for sed is a period, so it looks confusing.  http://docstore.mik.ua/orelly/unix/sedawk/ch03_02.htm
echo "     Amount of time it took to find them all (mm:ss): `cat /tmp/PTHC-Find-Homepages.txt | sed 's/\...//g'`."
echo
echo
echo


# STEP 2

# check all the sites (with "wget --spider") to see if the URL's are either good or broken

echo "2) Checking each HOMEPAGE in the Portage tree to see if it is accessible..."
echo
echo "     This process will also take a while."
echo
echo "     You can monitor the progress by running this command in another terminal:"
echo "        tail -f /tmp/PortageHomepagesTested.txt"
echo

# First, we'll remove this file if it's still there from a previous run (especially an aborted previous run)
rm -f /tmp/PortageHomepagesTested.txt > /dev/null




# Here we will have the wget command not use -o but rather -a, so that each wget doesn't overwrite the file, but instead appends to it

# Run 3 instances of wget in parallel, because otherwise this process takes forever.
# Idea taken from http://www.linuxjournal.com/content/downloading-entire-web-site-wget#comment-325493


# NOTE:  you can increase speed by adding more instances of wget (change "-P 3" to something like "-P 15")
#     but some DNS servers don't like you doing so many lookups so quickly, after a while it may stop resolving
#     So, if you want to increase parallelization further, it might be best to run your own DNS server like unbound

# Again, we will time this process.
/usr/bin/time -f %E -o /tmp/PTHC-Check-Each-Homepage.txt cat /tmp/PortageHomepages.txt | xargs -n1 -P 3 -i wget --spider -nv -a /tmp/PortageHomepagesTested.txt --timeout=10 --tries=3 --waitretry=10 --no-check-certificate --no-cookies -O /dev/null {}


echo "     Amount of time this step took (mm:ss): `cat /tmp/PTHC-Check-Each-Homepage.txt | sed 's/\...//g'`"
echo


echo
echo
echo "3) Processing the results..."
echo
echo



# STEP 3

# Remove the websites without issues ('200 OK' HTTP status code) from the list
#	and get the HTTP codes all the HOMEPAGES that have issues.


# First, we'll remove this file if it's still there from a previous run (especially an aborted previous run)
rm -f /tmp/PortageHomepagesLogs.txt > /dev/null

# First Remove all the "200" lines ("OK"), so that we are left with only the problem sites.
#   Then we get the error code for the sites that have issues
#   And we want to keep only the "http" lines, and remove the ":" that wget puts in at the end of the lines.


echo "   *PART 1* in progress: removing the valid HOMEPAGES from the list..."

/usr/bin/time -f %E -o /tmp/PTHC-Processing-Results-1.txt grep -v '200 OK' /tmp/PortageHomepagesTested.txt | grep -v '200 Ok' | grep http | sed 's/:$//g' > /tmp/PortageHomepagesWithIssues.txt
echo "     Amount of time *PART 1* of this step took: `cat /tmp/PTHC-Processing-Results-1.txt`"
echo
echo
echo "   *PART 2* in progress: identifying what problems the remaining HOMEPAGES's have..."
echo
echo "     You can monitor *PART 2* with this command:"
echo "        tail -f /tmp/PortageHomepagesLogs.txt"
echo 
echo 

/usr/bin/time -f %E -o /tmp/PTHC-Processing-Results-2.txt cat /tmp/PortageHomepagesWithIssues.txt | xargs -n1 -P 3 -i wget --timeout=10 --tries=3 --waitretry=10 --no-check-certificate --no-cookies -nv -a /tmp/PortageHomepagesLogs.txt -O /dev/null {}
echo "     Amount of time *PART 2* of this step took: `cat /tmp/PTHC-Processing-Results-2.txt`"



# Format for /tmp/PortageHomepagesLogs.txt is:   URL on one line, the issue is on the next line, repeat.
# Look for each type of issue, get the line that identifies the issue, also grab the preceding,
#    then remove the lines that ID the issue, then make sure to get lines that only start with a URL
#    then remove lines that have '%' which must (?) be a carryover from the ebuild
#    then remove the colons at the end, which wget adds in.

echo
echo "     *PART 3* in progress:  Sorting the HOMEPAGES according to their problems..."
echo

grep '403: Forbidden' /tmp/PortageHomepagesLogs.txt -B 2 | grep -v '403: Forbidden' | grep -E '^http' | grep -v '%' | sed s/:$//g > /tmp/PTHC-403.txt

grep '404: Not Found' /tmp/PortageHomepagesLogs.txt -B 2 | grep -v '404: Not Found' | grep -E '^http' | grep -v '%' | sed s/:$//g > /tmp/PTHC-404.txt

grep '500: Internal Server Error' /tmp/PortageHomepagesLogs.txt -B 2 | grep -v '500: Internal Server Error' | grep -E '^http' | grep -v '%' | sed s/:$//g > /tmp/PTHC-500.txt

grep '503: Service Unavailable' /tmp/PortageHomepagesLogs.txt -B 2 | grep -v '503: Service Unavailable' | grep -E '^http' | grep -v '%' | sed s/:$//g > /tmp/PTHC-503.txt

grep '410: Gone' /tmp/PortageHomepagesLogs.txt -B 2 | grep -v '410: Gone' | grep -E '^http' | grep -v '%' | sed s/:$//g > /tmp/PTHC-410.txt


# Should the next line be using 'PortageHomepages.txt' instead of  'PortageHomepagesLogs' ? 

grep 'unable to resolve host address' /tmp/PortageHomepagesLogs.txt -B 2 | grep -v 'unable to resolve host address' | grep -E '^http' | grep -v '%' | sed s/:$//g > /tmp/PTHC-DNS.txt



# STEP 7

# Display the useful information.

echo
echo
echo "4) Producing Results..."
echo


# REPORT EBUILDS WITH DNS ISSUES
echo
echo "  --> There are `wc -l /tmp/PTHC-DNS.txt | awk '{print $1}'` HOMEPAGES that have *DNS* issues."
echo ""
echo "     The URL's, and the ebuilds that contain them, are being recorded in"
echo "       /tmp/PTHC-DNS-Results.txt..."
echo 
rm -f /tmp/PTHC-DNS-Results.txt > /dev/null

echo "     There are `wc -l /tmp/PTHC-DNS.txt | awk '{print $1}'` HOMEPAGES that have *DNS* issues." >> /tmp/PTHC-DNS-Results.txt
echo >> /tmp/PTHC-DNS-Results.txt
echo "Need to fix: currently, all ebuilds that reference this URL will be included, whether or not it is the HOMEPAGE variable or the SRC_URI variable. No distinction is made in the output below, even though it says \"HOMEPAGE with a problem\"" >> /tmp/PTHC-DNS-Results.txt 
echo "" >> /tmp/PTHC-DNS-Results.txt

for i in $(cat /tmp/PTHC-DNS.txt) ; do
echo "" >> /tmp/PTHC-DNS-Results.txt
echo "HOMEPAGE with a problem...    $i" >> /tmp/PTHC-DNS-Results.txt
echo "   ...Searching Portage tree for all ebuilds that use this HOMEPAGE." >> /tmp/PTHC-DNS-Results.txt
echo "" >> /tmp/PTHC-DNS-Results.txt
find /usr/portage/ -name '*.ebuild' -type f -print | xargs -i grep -l $i {} >> /tmp/PTHC-DNS-Results.txt
echo "" >> /tmp/PTHC-DNS-Results.txt
echo "" >> /tmp/PTHC-DNS-Results.txt
done




# REPORT EBUILDS WITH 403 ISSUES

echo
echo
echo "  --> There are `wc -l /tmp/PTHC-403.txt | awk '{print $1}'` HOMEPAGES that have *403 Forbidden* issues."
echo ""
echo "     The URL's, and the ebuilds that contain them, are being recorded in"
echo "       /tmp/PTHC-403-Results.txt..."
echo 
rm -f /tmp/PTHC-403-Results.txt > /dev/null

echo "     There are `wc -l /tmp/PTHC-403.txt | awk '{print $1}'` HOMEPAGES that have *403 Forbidden* issues." >> /tmp/PTHC-403-Results.txt
echo >> /tmp/PTHC-403-Results.txt
echo "Need to fix: currently, all ebuilds that reference this URL will be included, whether or not it is the HOMEPAGE variable or the SRC_URI variable. No distinction is made in the output below, even though it says HOMEPAGE with a problem" >> /tmp/PTHC-403-Results.txt 
echo "" >> /tmp/PTHC-403-Results.txt

for i in $(cat /tmp/PTHC-403.txt) ; do
echo "" >> /tmp/PTHC-403-Results.txt
echo "HOMEPAGE with a problem...    $i" >> /tmp/PTHC-403-Results.txt
echo "   ...Searching Portage tree for all ebuilds that use this HOMEPAGE." >> /tmp/PTHC-403-Results.txt
echo "" >> /tmp/PTHC-403-Results.txt
find /usr/portage/ -name '*.ebuild' -type f -print | xargs -i grep -l $i {} >> /tmp/PTHC-403-Results.txt
echo "" >> /tmp/PTHC-403-Results.txt
echo "" >> /tmp/PTHC-403-Results.txt
done



# REPORT EBUILDS WITH 404 ISSUES
echo
echo
echo
echo "  --> There are `wc -l /tmp/PTHC-404.txt | awk '{print $1}'` HOMEPAGES that have *404 Not Found* issues."
echo ""
echo ""
echo "     The URL's, and the ebuilds that contain them, are being recorded in"
echo "       /tmp/PTHC-404-Results.txt..."
echo 
rm -f /tmp/PTHC-404-Results.txt > /dev/null

echo "     There are `wc -l /tmp/PTHC-404.txt | awk '{print $1}'` HOMEPAGES that have *404 Not Found* issues." >> /tmp/PTHC-404-Results.txt
echo >> /tmp/PTHC-404-Results.txt
echo "Need to fix: currently, all ebuilds that reference this URL will be included, whether or not it is the HOMEPAGE variable or the SRC_URI variable. No distinction is made in the output below, even though it says HOMEPAGE with a problem" >> /tmp/PTHC-404-Results.txt 
echo "" >> /tmp/PTHC-404-Results.txt

for i in $(cat /tmp/PTHC-404.txt) ; do
echo "" >> /tmp/PTHC-404-Results.txt
echo "HOMEPAGE with a problem...    $i" >> /tmp/PTHC-404-Results.txt
echo "   ...Searching Portage tree for all ebuilds that use this HOMEPAGE." >> /tmp/PTHC-404-Results.txt
echo "" >> /tmp/PTHC-404-Results.txt
find /usr/portage/ -name '*.ebuild' -type f -print | xargs -i grep -l $i {} >> /tmp/PTHC-404-Results.txt
echo "" >> /tmp/PTHC-404-Results.txt
echo "" >> /tmp/PTHC-404-Results.txt
done



# REPORT EBUILDS WITH 500 ISSUES
echo
echo
echo
echo "  --> There are `wc -l /tmp/PTHC-500.txt | awk '{print $1}'` HOMEPAGES that have *500 Internal Server Error* issues."
echo ""
echo ""
echo "     The URL's, and the ebuilds that contain them, are being recorded in"
echo "       /tmp/PTHC-500-Results.txt..."
echo 
rm -f /tmp/PTHC-500-Results.txt > /dev/null

echo "     There are `wc -l /tmp/PTHC-500.txt | awk '{print $1}'` HOMEPAGES that have *500 Internal Server Error* issues." >> /tmp/PTHC-404-Results.txt
echo >> /tmp/PTHC-500-Results.txt
echo "Need to fix: currently, all ebuilds that reference this URL will be included, whether or not it is the HOMEPAGE variable or the SRC_URI variable. No distinction is made in the output below, even though it says HOMEPAGE with a problem" >> /tmp/PTHC-500-Results.txt 
echo "" >> /tmp/PTHC-500-Results.txt

for i in $(cat /tmp/PTHC-500.txt) ; do
echo "" >> /tmp/PTHC-500-Results.txt
echo "HOMEPAGE with a problem...    $i" >> /tmp/PTHC-500-Results.txt
echo "   ...Searching Portage tree for all ebuilds that use this HOMEPAGE." >> /tmp/PTHC-500-Results.txt
echo "" >> /tmp/PTHC-500-Results.txt
find /usr/portage/ -name '*.ebuild' -type f -print | xargs -i grep -l $i {} >> /tmp/PTHC-500-Results.txt
echo "" >> /tmp/PTHC-500-Results.txt
echo "" >> /tmp/PTHC-500-Results.txt
done



# REPORT EBUILDS WITH 503 ISSUES
echo
echo
echo
echo "  --> There are `wc -l /tmp/PTHC-503.txt | awk '{print $1}'` HOMEPAGES that have *503 Service Unavailable* issues."
echo ""
echo ""
echo "     The URL's, and the ebuilds that contain them, are being recorded in"
echo "       /tmp/PTHC-503-Results.txt..."
echo 
rm -f /tmp/PTHC-503-Results.txt > /dev/null

echo "     There are `wc -l /tmp/PTHC-503.txt | awk '{print $1}'` HOMEPAGES that have *503 Service Unavailable* issues." >> /tmp/PTHC-503-Results.txt
echo >> /tmp/PTHC-503-Results.txt
echo "Need to fix: currently, all ebuilds that reference this URL will be included, whether or not it is the HOMEPAGE variable or the SRC_URI variable. No distinction is made in the output below, even though it says HOMEPAGE with a problem" >> /tmp/PTHC-503-Results.txt 
echo "" >> /tmp/PTHC-503-Results.txt

for i in $(cat /tmp/PTHC-503.txt) ; do
echo "" >> /tmp/PTHC-503-Results.txt
echo "HOMEPAGE with a problem...    $i" >> /tmp/PTHC-503-Results.txt
echo "   ...Searching Portage tree for all ebuilds that use this HOMEPAGE." >> /tmp/PTHC-503-Results.txt
echo "" >> /tmp/PTHC-503-Results.txt
find /usr/portage/ -name '*.ebuild' -type f -print | xargs -i grep -l $i {} >> /tmp/PTHC-503-Results.txt
echo "" >> /tmp/PTHC-503-Results.txt
echo "" >> /tmp/PTHC-503-Results.txt
done



# REPORT EBUILDS WITH 410 ISSUES
echo
echo
echo "  --> There are `wc -l /tmp/PTHC-410.txt | awk '{print $1}'` HOMEPAGES that have *410 Gone* issues."
echo ""
echo ""
echo "     The URL's, and the ebuilds that contain them, are being recorded in"
echo "       /tmp/PTHC-410-Results.txt..."
echo 
rm -f /tmp/PTHC-410-Results.txt > /dev/null

echo "     There are `wc -l /tmp/PTHC-410.txt | awk '{print $1}'` HOMEPAGES that have *410 Gone* issues." >> /tmp/PTHC-410-Results.txt
echo >> /tmp/PTHC-410-Results.txt
echo "Need to fix: currently, all ebuilds that reference this URL will be included, whether or not it is the HOMEPAGE variable or the SRC_URI variable. No distinction is made in the output below, even though it says HOMEPAGE with a problem" >> /tmp/PTHC-410-Results.txt 
echo "" >> /tmp/PTHC-410-Results.txt

for i in $(cat /tmp/PTHC-410.txt) ; do
echo "" >> /tmp/PTHC-410-Results.txt
echo "HOMEPAGE with a problem...    $i" >> /tmp/PTHC-410-Results.txt
echo "   ...Searching Portage tree for all ebuilds that use this HOMEPAGE." >> /tmp/PTHC-410-Results.txt
echo "" >> /tmp/PTHC-410-Results.txt
find /usr/portage/ -name '*.ebuild' -type f -print | xargs -i grep -l $i {} >> /tmp/PTHC-410-Results.txt
echo "" >> /tmp/PTHC-410-Results.txt
echo "" >> /tmp/PTHC-410-Results.txt
done




# Last step of cleanup
# rm /tmp/RealPortageHomepageIssues*
# rm /tmp/PortageHome*



echo  "This program started running at:  [date/time]"
echo  "It is now finished running at: `date`."
echo
echo


echo
echo
echo 
echo "5) Follow-up..."
echo
echo "     Now you can file bug reports: http://bugs.gentoo.org/enter_bug.cgi?product=Gentoo%20Linux&format=guided"
echo 
echo "		(first, double-check that someone hasn't filed a bug already"
echo "       AND double-check that a newer version of the ebuild hasn't fixed the broken HOMEPAGE already)"
echo
echo
echo "     Select \"Applications\" and enter a Description like \"Invalid HOMEPAGE for [package name]\"."
echo
echo "     Mention something like: \"The HOMEPAGE will not load because [what type of error].  The HOMEPAGE is:  [URL].\"" 
echo
echo
